{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "stretch-assets",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/beelzebruno/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "from gensim import corpora\n",
    "from gensim import similarities\n",
    "from gensim import models\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-harvey",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis with Star Wars movie scripts\n",
    "\n",
    "This is a very interesting study of natural language processing and the application of the Latent Semantic Analysis technic for text based search engine with modern AI solutions.\n",
    "\n",
    "I have collected the full Star Wars movies scripts from the episodes I to VI at https://imsdb.com/search.php and grouped the documents as text files and separated in two different directories grouping the movie scripts by its chronbological order, the episodes I, II and III in a folder named `sw_prequels` and the first original trilogy (IV, V, VI) in a folder named `sw_original_trilogy`, giving a structure like this:\n",
    "\n",
    "```\n",
    ".\n",
    "├── lsa.ipynb\n",
    "├── sw_original_trilogy\n",
    "│   ├── IV_a_new_hope.txt\n",
    "│   ├── VI_return_of_the_jedi.txt\n",
    "│   └── V_the_empire_strikes_back.txt\n",
    "└── sw_prequels\n",
    "    ├── III_revenge_of_the_sith.txt\n",
    "    ├── II_attack_of_the_clones.txt\n",
    "    └── I_the_phantom_menace.txt\n",
    "\n",
    "2 directories, 7 files\n",
    "```\n",
    "\n",
    "The point is to fit the algorithm with the content of the texts from the prequel movies, located in a directory and the content of the texts from the original trilogy in the another directory and when the algotithm receive a text query as input it has to match the inputed text to both prequel and original trilogy, resulting on the probability of the existance of the text in one of the directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "athletic-gambling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sw_prequels', 'sw_original_trilogy', 'lsa.ipynb', '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start by listing the current directory\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "governing-scholarship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['III_revenge_of_the_sith.txt', 'II_attack_of_the_clones.txt', '.ipynb_checkpoints', 'I_the_phantom_menace.txt']\n",
      "['V_the_empire_strikes_back.txt', 'VI_return_of_the_jedi.txt', '.ipynb_checkpoints', 'IV_a_new_hope.txt']\n"
     ]
    }
   ],
   "source": [
    "# List the files inside the target directories\n",
    "print(os.listdir('sw_prequels'))\n",
    "print(os.listdir('sw_original_trilogy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fourth-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all files of each directory as string content in separated lists\n",
    "prequels = []\n",
    "trilogy = []\n",
    "\n",
    "for file in os.listdir('sw_prequels'):\n",
    "    if not file.endswith('.txt'): continue\n",
    "    with open(f'sw_prequels/{file}', 'r') as f:\n",
    "        prequels.extend(f.read().split('\\n'))\n",
    "\n",
    "for file in os.listdir('sw_original_trilogy'):\n",
    "    if not file.endswith('.txt'): continue\n",
    "    with open(f'sw_original_trilogy/{file}', 'r') as f:\n",
    "        trilogy.extend(f.read().split('\\n'))\n",
    "\n",
    "# Remove the blank strings '' and tabs \\t left from line spliting\n",
    "prequels = [line.replace('\\t', '').lower() for line in prequels]\n",
    "prequels = list(filter(('').__ne__, prequels))\n",
    "\n",
    "trilogy = [line.replace('\\t', '').lower() for line in trilogy]\n",
    "trilogy = list(filter(('').__ne__, trilogy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "greek-jenny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9159\n",
      "13823\n"
     ]
    }
   ],
   "source": [
    "# Total lines of each sequence\n",
    "print(len(prequels))\n",
    "print(len(trilogy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "lonely-jones",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artoo squeals in a panic. on the view screen artoo\\'s squeal reads out, \"we\\'re not going to make it.\"',\n",
       " 'anakin: wrong thought, artoo.',\n",
       " 'anakin slips through the narrow gap. the trailing vulture droid fighters crash.',\n",
       " \"anakin: (continuing) i'm through.\",\n",
       " 'obi-wan continues to fire on the vulture droid fighters, driving them into the explosion.',\n",
       " 'a clone fighter is hit and explodes, spewing debris. the clone pilot spins off into space.',\n",
       " 'finally, obi-wan peels off and swings around, pulling up alongside anakin. clone fight squad seven battles the droids.',\n",
       " 'odd ball: there are too many of them.',\n",
       " \"clone pilot 2: i'm on your wing. break left. break left. they're all over me. get them off my . . .\",\n",
       " \"anakin: i'm going to go help them out!\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview some sentences fom the prequels\n",
    "prequels[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "editorial-target",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['solo?',\n",
       " 'han',\n",
       " 'no sign of life out there, general.',\n",
       " \"the sensors are in place.  you'll \",\n",
       " 'know if anything comes around.',\n",
       " 'rieekan',\n",
       " 'commander skywalker reported in yet?',\n",
       " 'han',\n",
       " \"no.  he's checking out a meteorite \",\n",
       " 'that hit near him.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview some sentences from the OT\n",
    "trilogy[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "frank-actor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# transform all prequel movie scripts into one single document (a big string)\n",
    "prequels_doc = ' '.join(prequels)\n",
    "# same to the original trilogy\n",
    "trilogy_doc = ' '.join(trilogy)\n",
    "# merge both documents in list of two rows, one for each document\n",
    "docs = [prequels_doc, trilogy_doc]\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "pending-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The index 0 is a prequel content, and the index 1 is from original trilogy\n",
    "translate = {\n",
    "    0: 'PREQUELS',\n",
    "    1: 'ORIGINAL TRILOGY'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "white-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the stop words\n",
    "txts = [[word for word in document.lower().split()\n",
    "         if word not in stop_words]\n",
    "         for document in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "worst-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating frequency\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "for text in txts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Removing unique tokens\n",
    "txts = [[token for token in text if frequency[token] > 1] for text in txts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "expanded-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use gensim to get a numeric vector representation of the texts\n",
    "gensim_dictionary = corpora.Dictionary(txts)\n",
    "gensim_corpus = [gensim_dictionary.doc2bow(text) for text in txts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "duplicate-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is only two topics: The Sequel and The OT\n",
    "lsi = models.LsiModel(gensim_corpus, id2word=gensim_dictionary, num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "destroyed-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(query):\n",
    "    # create a bag of words from query input\n",
    "    vec_bow = gensim_dictionary.doc2bow(query.lower().split())\n",
    "    vec_lsi = lsi[vec_bow]\n",
    "\n",
    "    # transforming corpus to LSI space and index it\n",
    "    index = similarities.MatrixSimilarity(lsi[gensim_corpus])\n",
    "\n",
    "    # Perform a similarity query against the corpus\n",
    "    simil = index[vec_lsi]  \n",
    "    simil = sorted(list(enumerate(simil)), key=lambda item: -item[1])\n",
    "\n",
    "    topic1, topic2 = simil\n",
    "    result = {\n",
    "        translate[topic1[0]]: topic1[1],\n",
    "        translate[topic2[0]]: topic2[1]\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "modern-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some test messages\n",
    "query_inputs = [\n",
    "    'yes captain',  # prequels (Clones acknowledgement (II, III))\n",
    "    'you where the chosen one',  # prequels Obi-Wan to Anakin (III)\n",
    "    'no i am your father',  # OT (Darth Vader to Luke (V))\n",
    "    'this is the fastest ship of the galaxy',  # OT Han Solo about the Millenium Falcon\n",
    "    'its your imagination kid, come on lets have sme optimism here',  # OT Han Solo to Luke (VI)\n",
    "    'of course i know him, its me',  # OT (Ben Kenobi to Luke (IV))\n",
    "    'this is a system we cannot afford to loose',  # prequels (Jedi council meeting (III))\n",
    "    'this will make a fine addition to my collection',  # prequels (General Grievous getting a Jedi lightsaber (III))\n",
    "    'an elegant weapon for a more civilized age',  # OT (Ben Kenobi giving Luke his father lightsaber (IV))\n",
    "    'red five standing by',  #  OT (Luke as X-wing pilot in rebel attack against the deathstar (IV))\n",
    "    'with a million more well on the way',  # prequels (Kaminoan to Obi-Wan (II))\n",
    "    'i will rearrange the senate in a new galactic empire'  # prequels (Palpatine electing himself as Emperor) (III)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "overhead-colleague",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes captain\n",
      "PREQUELS: 0.9981762170791626\n",
      "ORIGINAL TRILOGY: 0.36091670393943787\n",
      "-------------------------\n",
      "you where the chosen one\n",
      "PREQUELS: 0.8601295351982117\n",
      "ORIGINAL TRILOGY: 0.74738609790802\n",
      "-------------------------\n",
      "no i am your father\n",
      "ORIGINAL TRILOGY: 0.9811432361602783\n",
      "PREQUELS: 0.11409109830856323\n",
      "-------------------------\n",
      "this is the fastest ship of the galaxy\n",
      "ORIGINAL TRILOGY: 0.8635291457176208\n",
      "PREQUELS: 0.7429161071777344\n",
      "-------------------------\n",
      "its your imagination kid, come on lets have sme optimism here\n",
      "ORIGINAL TRILOGY: 0.9800994396209717\n",
      "PREQUELS: 0.48702582716941833\n",
      "-------------------------\n",
      "of course i know him, its me\n",
      "ORIGINAL TRILOGY: 0.8212767839431763\n",
      "PREQUELS: 0.7931703925132751\n",
      "-------------------------\n",
      "this is a system we cannot afford to loose\n",
      "PREQUELS: 0.9442289471626282\n",
      "ORIGINAL TRILOGY: 0.6007170081138611\n",
      "-------------------------\n",
      "this will make a fine addition to my collection\n",
      "PREQUELS: 0.8419355154037476\n",
      "ORIGINAL TRILOGY: 0.76996248960495\n",
      "-------------------------\n",
      "an elegant weapon for a more civilized age\n",
      "ORIGINAL TRILOGY: 0.9981638193130493\n",
      "PREQUELS: 0.36110761761665344\n",
      "-------------------------\n",
      "red five standing by\n",
      "ORIGINAL TRILOGY: 0.9985909461975098\n",
      "PREQUELS: 0.2529756426811218\n",
      "-------------------------\n",
      "with a million more well on the way\n",
      "PREQUELS: 0.8755751848220825\n",
      "ORIGINAL TRILOGY: 0.7263645529747009\n",
      "-------------------------\n",
      "i will rearrange the senate in a new galactic empire\n",
      "PREQUELS: 0.9702271223068237\n",
      "ORIGINAL TRILOGY: 0.5256475210189819\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for query in query_inputs:\n",
    "    out = get_result(query)\n",
    "    print(query)\n",
    "    for key, value in out.items():\n",
    "        print(f'{key}: {value}')\n",
    "    print('-'*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-bulgarian",
   "metadata": {},
   "source": [
    "For this simple quotes the search engine seems to be pretty accurate in figure out from which season the message belongs to, but it may comes to miss or wrongly classify a lot o inputs. LSA algorithms seems to works better on very large datasets, and also for many other applications as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-webmaster",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Based on this code recipe: https://www.projectpro.io/recipes/explain-similarity-queries-gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-constitutional",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
