{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "innovative-address",
   "metadata": {},
   "source": [
    "# sklearn-assignment\n",
    "\n",
    "### Bruno Luvizotto Carli\n",
    "\n",
    "### IAA2021 - UFPR\n",
    "\n",
    "Atividade proposta na disciplina de Linguagem de Programação Aplicada do curso de Especialização em Inteligência Artfical Aplicada da Universidade Federal do Paraná (UFPR).\n",
    "\n",
    "### Professor: Alexander Robert Kutzke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-triangle",
   "metadata": {},
   "source": [
    "# Original Exercise Author: Olivier Grisel\n",
    "\n",
    "<olivier.grisel@ensta.org>\n",
    "\n",
    "## License: Simplified BSD\n",
    "\n",
    "\n",
    "Build a sentiment analysis / polarity model\n",
    "\n",
    "Sentiment analysis can be casted as a binary text classification problem,\n",
    "that is fitting a linear classifier on features extracted from the text\n",
    "of the user messages so as to guess wether the opinion of the author is\n",
    "positive or negative.\n",
    "\n",
    "In this examples we will use a movie review dataset.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-sympathy",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controlling-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise pre-loaded modules\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-receptor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional modules\n",
    "from sklearn import model_selection\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-truck",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "expired-negative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2000\n"
     ]
    }
   ],
   "source": [
    "# the training data folder must be passed as first argument\n",
    "movie_reviews_data_folder = r\"./data\"\n",
    "dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
    "print(\"n_samples: %d\" % len(dataset.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-transcript",
   "metadata": {},
   "source": [
    "#### Lets take a look at the data first.\n",
    "\n",
    "A Pandas Data Frame will be handful here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accessible-chick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'plot : two teen couples go to a church party...</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'the happy bastard\\'s quick movie review \\nda...</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b\"it is movies like these that make a jaded mo...</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b' \" quest for camelot \" is warner bros . \\' f...</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'synopsis : a mentally unstable man undergoin...</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>b\"wow ! what a movie . \\nit's everything a mov...</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>b'richard gere can be a commanding actor , but...</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>b'glory--starring matthew broderick , denzel w...</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>b'steven spielberg\\'s second epic film on worl...</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>b'truman ( \" true-man \" ) burbank is the perfe...</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Data Target\n",
       "0     b'plot : two teen couples go to a church party...   b'0'\n",
       "1     b'the happy bastard\\'s quick movie review \\nda...   b'0'\n",
       "2     b\"it is movies like these that make a jaded mo...   b'0'\n",
       "3     b' \" quest for camelot \" is warner bros . \\' f...   b'0'\n",
       "4     b'synopsis : a mentally unstable man undergoin...   b'0'\n",
       "...                                                 ...    ...\n",
       "1995  b\"wow ! what a movie . \\nit's everything a mov...   b'1'\n",
       "1996  b'richard gere can be a commanding actor , but...   b'1'\n",
       "1997  b'glory--starring matthew broderick , denzel w...   b'1'\n",
       "1998  b'steven spielberg\\'s second epic film on worl...   b'1'\n",
       "1999  b'truman ( \" true-man \" ) burbank is the perfe...   b'1'\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    data= np.c_[dataset.data, dataset.target],\n",
    "    columns=['Data', 'Target']\n",
    ")\n",
    "\n",
    "df  # show information on jupyter notebook output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-timer",
   "metadata": {},
   "source": [
    "## Split data in train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "material-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset in training and test set:\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    dataset.data, dataset.target, test_size=0.25, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "worth-coordinator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
    "# that are too rare or too frequent\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())\n",
    "                    ])\n",
    "\n",
    "text_clf.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-spiritual",
   "metadata": {},
   "source": [
    "## Building a filter\n",
    "\n",
    "Now we can write a sub-routine (a function) that uses the classifier and returns the\n",
    "filetred string and the removed tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "unexpected-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_filter(text: str, clf: Callable = text_clf.predict) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Remove all most/less common tokens from a text string.\n",
    "    Returns a tuple in which the first position corresponds\n",
    "    to the inputed string without the tokens and in the\n",
    "    second position a list of the removed tokens.\n",
    "    \"\"\"\n",
    "    tokens: List[str] = text.split()\n",
    "    removed: List[str] = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if clf([token])[0] == 0:\n",
    "            removed.append(tokens.pop(tokens.index(token)))\n",
    "\n",
    "    filtered: str = ' '.join(token for token in tokens).strip()\n",
    "    return filtered, list(set(removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-underwear",
   "metadata": {},
   "source": [
    "Awesome, we can get both a filtered text and also a list of the filtered tokens, lets take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "double-ceramic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the brown fox jumps the dog\n",
      "['lazy', 'over', 'quick']\n"
     ]
    }
   ],
   "source": [
    "filtered_text, removed_tokens = string_filter('the quick brown fox jumps over the lazy dog')\n",
    "print(filtered_text)\n",
    "print(removed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-journey",
   "metadata": {},
   "source": [
    "## Optimizing model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "virgin-penalty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf__use_idf True\n",
      "vect__ngram_range (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# TASK: Build a grid search to find out whether unigrams or bigrams are\n",
    "# more useful.\n",
    "# Fit the pipeline on the training set using grid search for the parameters\n",
    "\n",
    "# i will be using the standartd example from the docs.\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(docs_train, y_train)\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(param_name, gs_clf.best_params_[param_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-ordinary",
   "metadata": {},
   "source": [
    "Lets test our filter with this new classifier, the function defined above accepts an caller as parameter, so we can test it with different estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "brown-century",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the brown fox jumps the dog\n",
      "['lazy', 'over', 'quick']\n"
     ]
    }
   ],
   "source": [
    "filtered_text, removed_tokens = string_filter('the quick brown fox jumps over the lazy dog', gs_clf.predict)\n",
    "print(filtered_text)\n",
    "print(removed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-mounting",
   "metadata": {},
   "source": [
    "Looks like it works as well as the previous classifier, but this alone dont tell us much about the model performance, so lets check out the mean test score of the both classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "naughty-murray",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before the GridSearch:  0.824\n",
      "After GridSearch:  0.844\n"
     ]
    }
   ],
   "source": [
    "print('Before the GridSearch: ', np.mean(text_clf.predict(docs_test) == y_test))\n",
    "print('After GridSearch: ', np.mean(gs_clf.predict(docs_test) == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-queen",
   "metadata": {},
   "source": [
    "We can see that the GridSearch slightly optimized the accuracy by choosing a better parameter combination. Lets take a look on the best score found by the GridSearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "removable-integer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8246666666666667\n"
     ]
    }
   ],
   "source": [
    "# TASK: print the cross-validated scores for the each parameters set\n",
    "# explored by the grid search\n",
    "print(gs_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "signal-detector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.87      0.82      0.84       257\n",
      "         pos       0.82      0.87      0.84       243\n",
      "\n",
      "    accuracy                           0.84       500\n",
      "   macro avg       0.84      0.84      0.84       500\n",
      "weighted avg       0.85      0.84      0.84       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TASK: Predict the outcome on the testing set and store it in a variable\n",
    "# named y_predicted\n",
    "y_predicted = gs_clf.predict(docs_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(metrics.classification_report(y_test, y_predicted,\n",
    "                                    target_names=dataset.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "experienced-premium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[210  47]\n",
      " [ 31 212]]\n"
     ]
    }
   ],
   "source": [
    "# Print and plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "lined-binding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-merchandise",
   "metadata": {},
   "source": [
    "## Test other estimators performance\n",
    "\n",
    "We can train other models and find out whic suits better over the data we have. There are a saying that state \"there are no better model than others, but there is a model that performs better on a given dataset\", this is commonly known as **no free lunch**. Lets taste some other models on our pipeline to figure out if there is a better estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "manual-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the models we will test\n",
    "models = [DummyClassifier, LogisticRegression, DecisionTreeClassifier, KNeighborsClassifier, MultinomialNB, LinearSVC]\n",
    "\n",
    "def no_free_lunch():\n",
    "    \"\"\"\n",
    "    Tests the score for different models. A model will not fit on every data\n",
    "    samples but certainly, some will show better performance.\n",
    "    Returns a dict with the trained model.\n",
    "    \"\"\"\n",
    "    trained = {}\n",
    "    for model in models:\n",
    "        cls = Pipeline(\n",
    "            [('vect', CountVectorizer()),\n",
    "             ('tfidf', TfidfTransformer()),\n",
    "             ('clf', model())\n",
    "        ])\n",
    "        cls.fit(docs_train, y_train)\n",
    "        parameters = {\n",
    "            'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "            'tfidf__use_idf': (True, False),\n",
    "        }\n",
    "        cls = GridSearchCV(cls, parameters, n_jobs=-1)\n",
    "        cls = cls.fit(docs_train, y_train)\n",
    "\n",
    "        # test prediction\n",
    "        y_pred = cls.predict(docs_test)\n",
    "\n",
    "        # scores\n",
    "        precision_score = metrics.precision_score(\n",
    "            y_test, y_pred, average='weighted', labels=np.unique(y_pred)\n",
    "        )\n",
    "        f1_score = metrics.f1_score(\n",
    "            y_test, y_pred, average=\"weighted\", labels=np.unique(y_pred)\n",
    "        )\n",
    "        trained[model.__name__] = cls\n",
    "        print(f'{model.__name__:22}\\n\\ttest set score: {np.mean(y_pred == y_test):.2f}')\n",
    "        print(f'\\tPrecision score: {precision_score}')\n",
    "        print(f'\\tf1 score: {f1_score}')\n",
    "\n",
    "    print('_'*50)\n",
    "    return trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "textile-hayes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyClassifier       \n",
      "\ttest set score: 0.49\n",
      "\tPrecision score: 0.486\n",
      "\tf1 score: 0.6541049798115747\n",
      "LogisticRegression    \n",
      "\ttest set score: 0.84\n",
      "\tPrecision score: 0.8376723113154648\n",
      "\tf1 score: 0.8359790076641227\n",
      "DecisionTreeClassifier\n",
      "\ttest set score: 0.60\n",
      "\tPrecision score: 0.6037507828181453\n",
      "\tf1 score: 0.60372067522724\n",
      "KNeighborsClassifier  \n",
      "\ttest set score: 0.58\n",
      "\tPrecision score: 0.6721185750195025\n",
      "\tf1 score: 0.5323893048128342\n",
      "MultinomialNB         \n",
      "\ttest set score: 0.82\n",
      "\tPrecision score: 0.8244894798316773\n",
      "\tf1 score: 0.8240337968667488\n",
      "LinearSVC             \n",
      "\ttest set score: 0.87\n",
      "\tPrecision score: 0.87555967710936\n",
      "\tf1 score: 0.8739924399697598\n",
      "__________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trained_models = no_free_lunch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-genesis",
   "metadata": {},
   "source": [
    "We can see that some models perform better and some perform worse.\n",
    "Also note that, we are printing not only the model `score`, but the `precision` and the `f1` score.\n",
    "\n",
    "We can see that the LinearSVC performs a little better than our previously MultinomialNB, and the DummyClassifier has worse score and precision.\n",
    "\n",
    "Lets test the filtering function on each model, lets see if the function behavior changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "difficult-holiday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________\n",
      "DummyClassifier\n",
      "the quick brown fox jumps over the lazy dog\n",
      "[]\n",
      "__________________________________________________\n",
      "LogisticRegression\n",
      "the brown jumps the dog\n",
      "['fox', 'over', 'quick', 'lazy']\n",
      "__________________________________________________\n",
      "DecisionTreeClassifier\n",
      "the quick brown fox jumps over the lazy dog\n",
      "[]\n",
      "__________________________________________________\n",
      "KNeighborsClassifier\n",
      "the quick brown fox jumps the dog\n",
      "['lazy', 'over']\n",
      "__________________________________________________\n",
      "MultinomialNB\n",
      "the brown fox jumps the dog\n",
      "['lazy', 'over', 'quick']\n",
      "__________________________________________________\n",
      "LinearSVC\n",
      "the brown fox over the dog\n",
      "['lazy', 'quick', 'jumps']\n",
      "__________________________________________________\n"
     ]
    }
   ],
   "source": [
    "text = 'the quick brown fox jumps over the lazy dog'\n",
    "for model_name, classifier in trained_models.items():\n",
    "    filtered_text, removed_tokens = string_filter(text, classifier.predict)\n",
    "    print('_'*50)\n",
    "    print(model_name)\n",
    "    print(filtered_text)\n",
    "    print(removed_tokens)\n",
    "print('_'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-portugal",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This reveals different ways of applying a filter, some models such as the Decision Tree and the Dummy Classifier dont fits well, and the others have peculiar distinctions. The application purpose should define which model is better to deploy with the final application, since all models seems to behave different from each other."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
