{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "noble-developer",
   "metadata": {},
   "source": [
    "## Rede Neural Artificial\n",
    "\n",
    "<img src=\"https://media1.giphy.com/media/7HAm2aWDviqeQ/200.gif\" align=\"center\" width=\"200\">\n",
    "<div align=\"right\">Bruno L. Carli</div>\n",
    "<div align=\"right\">IAA2021 - UFPR</div>\n",
    "\n",
    "Exercício estudado na disciplina de **Introdução à Inteligência Artificial** do curso de Especialização em Inteligência Artfical Aplicada da Universidade Federal do Paraná (UFPR). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-mobility",
   "metadata": {},
   "source": [
    "Uma rede neural artificial é composta por unidades chamadas **neurônios**, cujos são representações computacionais dsa celulas do cérebro humano. A arquitetura do neurônio é ilustrada pela figura abaixo:\n",
    "\n",
    "![neuron](https://i.ibb.co/M1RjHd5/Captura-de-Tela-2021-03-07-a-s-15-07-20.png)\n",
    "\n",
    "\n",
    "Um neurônio recebe uma entrada de tamanho fixo, cujas são múltiplicadas por pesos definidos no neuônio e somadas a um viés em uma função de soma. O total é calculado em uma funçnao de ativação que poderá ou não disparar o neurônio.\n",
    "\n",
    "A função de soma:\n",
    "\n",
    "\\begin{equation}\n",
    "    u = \\sum_{i=1}^{m} {xi}\\times{wi}\n",
    "\\end{equation}\n",
    "\n",
    "Exemplos de funções de ativação:\n",
    "\n",
    "![activation](https://i.ibb.co/0XQPW0f/Captura-de-Tela-2021-03-07-a-s-15-11-22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-plain",
   "metadata": {},
   "source": [
    "## Exercício\n",
    "\n",
    "Seja a rede neural baixo:\n",
    "\n",
    "![exercicio](https://i.ibb.co/K700cYS/Captura-de-Tela-2021-03-07-a-s-15-13-13.png)\n",
    "\n",
    "Dada a entrada x1=-3, x2=1, dê os valores de saída de todos os neurônios e indique qual é a saída da rede.\n",
    "\n",
    "Os neurônios N1, N2 e N3 possuem função de ativação linear. Já N4 possui função de ativação tangente hiperbólica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-flood",
   "metadata": {},
   "source": [
    "### Fórmulas para as funções dos neurônios\n",
    "\n",
    "#### Função linear\n",
    "\n",
    "\\begin{equation}\n",
    "   f(x) = x\n",
    "\\end{equation}\n",
    "\n",
    "##### Tangente hiperbólica\n",
    "\n",
    "\\begin{equation}\n",
    "   tanh(t) = \\frac{e^t - e^{-t}}{e^t + e^{-t}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "narrative-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import e\n",
    "\n",
    "# Implementação das funções de ativação\n",
    "def linear_function(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def tanh(t):\n",
    "    return (e**t - e**-t) / (e**t + e**-t)\n",
    "\n",
    "function_map = {\n",
    "    'linear': linear_function,\n",
    "    'tanh': tanh\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "immediate-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação do Neurônio\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias=1, wbias=1, activation='linear', name=None):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.wbias = wbias\n",
    "        self.activation = function_map.get('linear', 'linear')  # lienar por padrão\n",
    "        self.name = name if name else id(self)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    def sum_function(self, inputs):\n",
    "        return sum([xi*wi for xi, wi in zip(inputs, self.weights)]) + (self.bias*self.wbias)\n",
    "\n",
    "    def execute(self, inputs):\n",
    "        out = self.activation(self.sum_function(inputs))\n",
    "        print('---------------------')\n",
    "        print(f'{self} - > {out}')\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "baking-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementação da rede neural\n",
    "class NeuralLayer:\n",
    "    def __init__(self, neurons, is_output=False):\n",
    "        self.neurons = neurons\n",
    "        self.is_output = is_output\n",
    "        self.next_layer = None\n",
    "\n",
    "    def forward_to(self, layer):\n",
    "        if self.is_output:\n",
    "            raise Exception('Output layer cannot forward signal.')\n",
    "\n",
    "        self.next_layer = layer\n",
    "\n",
    "    def execute_and_get_payload(self, inputs):\n",
    "        outputs = [n.execute(inputs) for n in self.neurons]\n",
    "        if not self.is_output and self.next_layer:\n",
    "            return self.next_layer.execute_and_get_payload(outputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, hidden_layers, output_layer):\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_layer = output_layer\n",
    "        self.connect_layers()\n",
    "\n",
    "    def connect_layers(self):\n",
    "        # conecta as camadas ocultas\n",
    "        for layer in self.hidden_layers:\n",
    "            try:\n",
    "                next_layer = self.hidden_layers[self.hidden_layers.index(layer) + 1]\n",
    "            except IndexError:\n",
    "                # conecta a camada de saida\n",
    "                self.hidden_layers[-1].forward_to(self.output_layer)\n",
    "            else:\n",
    "                layer.forward_to(next_layer)\n",
    "\n",
    "    def execute(self, inputs):\n",
    "        return self.hidden_layers[0].execute_and_get_payload(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "vanilla-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = Neuron([0.2, 0.8], wbias=0.1, name='N1')\n",
    "n2 = Neuron([0.1, 0.2], wbias=0.4, name='N2')\n",
    "n3 = Neuron([0.9, 0.5], wbias=0.2, name='N3')\n",
    "n4 = Neuron([0.9, 0.3, 0.3], wbias=0.1, activation='tanh', name='N4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "mighty-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeira camada\n",
    "layer_1 = NeuralLayer([n1, n2, n3])\n",
    "\n",
    "# Segunda (e última) camada\n",
    "output_layer = NeuralLayer([n4], is_output=True)\n",
    "\n",
    "# Conecta as camadas\n",
    "layer_1.forward_to(output_layer)\n",
    "\n",
    "# Encapsula as camadas em uma rede neural\n",
    "neural_network = NeuralNetwork([layer_1], output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "minus-stevens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "N1 - > 0.29999999999999993\n",
      "---------------------\n",
      "N2 - > 0.3\n",
      "---------------------\n",
      "N3 - > -2.0\n",
      "---------------------\n",
      "N4 - > -0.13999999999999999\n",
      "\n",
      "Saída final: -0.13999999999999999\n"
     ]
    }
   ],
   "source": [
    "output = neural_network.execute([-3, 1])\n",
    "print(f'\\nSaída final: {output[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
